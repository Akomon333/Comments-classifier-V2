{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akomon333/Comments-classifier-V2/blob/main/commentclassificationv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAD-80T4mTwG"
      },
      "outputs": [],
      "source": [
        "pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIuRzXoNgEOi",
        "outputId": "c237c11a-fb9b-4e75-b851-a35b064cda6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "import pandas as pd\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import EarlyStoppingCallback\n",
        "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVaIkcqAtpjA",
        "outputId": "7ed8b6cc-fdcd-4bc5-ed73-6b6dbf560a0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Comment': Value('string'), 'labels': Value('int64')}\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Datasets/YoutubeCommentsDataSet.csv\")\n",
        "dataset = Dataset.from_pandas(df).train_test_split(test_size=0.1, seed=42)\n",
        "dataset = dataset.rename_column(\"Sentiment\", \"labels\")\n",
        "print(dataset[\"train\"].features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sn39VCes9J2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    bal_acc = balanced_accuracy_score(labels, predictions)\n",
        "\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average=\"macro\"\n",
        "    )\n",
        "\n",
        "    precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average=None\n",
        "    )\n",
        "\n",
        "    class_names = [\"negative\", \"neutral\", \"positive\"]\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": acc,\n",
        "        \"balanced_accuracy\": bal_acc,\n",
        "        \"precision_macro\": precision_macro,\n",
        "        \"recall_macro\": recall_macro,\n",
        "        \"f1_macro\": f1_macro,\n",
        "    }\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        metrics[f\"precision_{class_name}\"] = precision_per_class[i]\n",
        "        metrics[f\"recall_{class_name}\"] = recall_per_class[i]\n",
        "        metrics[f\"f1_{class_name}\"] = f1_per_class[i]\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-n45Hc8hoUA"
      },
      "outputs": [],
      "source": [
        "# 0: negative, 1: neutral, 2: Positive\n",
        "\n",
        "# 2    10642\n",
        "# 1     3319\n",
        "# 0     2296\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    \"roberta-base\",\n",
        "    hidden_dropout_prob=0.15,\n",
        "    attention_probs_dropout_prob=0.15,\n",
        "    num_labels=3\n",
        ")\n",
        "\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', config=config)\n",
        "class_counts = torch.tensor([2296, 3319, 10642], dtype=torch.float)\n",
        "weights = 1.0 / class_counts\n",
        "weights = weights / weights.sum() * len(class_counts)\n",
        "weights = weights.to('cuda')\n",
        "loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "def focal_loss(logits, labels, gamma=2, weight=None):\n",
        "    ce_loss = F.cross_entropy(logits, labels, weight=weight, reduction='none')\n",
        "    pt = torch.exp(-ce_loss)\n",
        "    loss = ((1 - pt) ** gamma * ce_loss).mean()\n",
        "    return loss\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss = focal_loss(logits.view(-1, model.config.num_labels),\n",
        "                          labels.view(-1),\n",
        "                          gamma=1.6,\n",
        "                          weight=weights)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    tokens = tokenizer(\n",
        "        examples[\"Comment\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=200\n",
        "    )\n",
        "    tokens[\"labels\"] = examples[\"labels\"]\n",
        "    return tokens\n",
        "\n",
        "tokenized_train = dataset['train'].map(tokenize_function, batched=True, num_proc=4)\n",
        "tokenized_test = dataset['test'].map(tokenize_function, batched=True, num_proc=4)\n",
        "\n",
        "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "print(tokenized_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo6ecd3jrJbr"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "    max_grad_norm=1.0,\n",
        "    warmup_ratio=0.05,\n",
        "    lr_scheduler_type='linear',\n",
        "    fp16=True,\n",
        "    metric_for_best_model=\"balanced_accuracy\",\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "6Q053TSgn5qu",
        "outputId": "c0850c4a-729d-498b-f571-0e8779abb4c4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7316' max='7316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7316/7316 19:36, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Balanced Accuracy</th>\n",
              "      <th>Precision Macro</th>\n",
              "      <th>Recall Macro</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>Precision Negative</th>\n",
              "      <th>Recall Negative</th>\n",
              "      <th>F1 Negative</th>\n",
              "      <th>Precision Neutral</th>\n",
              "      <th>Recall Neutral</th>\n",
              "      <th>F1 Neutral</th>\n",
              "      <th>Precision Positive</th>\n",
              "      <th>Recall Positive</th>\n",
              "      <th>F1 Positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.124700</td>\n",
              "      <td>0.177669</td>\n",
              "      <td>0.876999</td>\n",
              "      <td>0.817481</td>\n",
              "      <td>0.831896</td>\n",
              "      <td>0.817481</td>\n",
              "      <td>0.816223</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.889952</td>\n",
              "      <td>0.814004</td>\n",
              "      <td>0.830645</td>\n",
              "      <td>0.598837</td>\n",
              "      <td>0.695946</td>\n",
              "      <td>0.915044</td>\n",
              "      <td>0.963653</td>\n",
              "      <td>0.938720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.171792</td>\n",
              "      <td>0.884994</td>\n",
              "      <td>0.849925</td>\n",
              "      <td>0.821524</td>\n",
              "      <td>0.849925</td>\n",
              "      <td>0.834026</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.875598</td>\n",
              "      <td>0.807947</td>\n",
              "      <td>0.756677</td>\n",
              "      <td>0.741279</td>\n",
              "      <td>0.748899</td>\n",
              "      <td>0.957895</td>\n",
              "      <td>0.932898</td>\n",
              "      <td>0.945231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.057500</td>\n",
              "      <td>0.241748</td>\n",
              "      <td>0.900369</td>\n",
              "      <td>0.851270</td>\n",
              "      <td>0.860712</td>\n",
              "      <td>0.851270</td>\n",
              "      <td>0.855209</td>\n",
              "      <td>0.824074</td>\n",
              "      <td>0.851675</td>\n",
              "      <td>0.837647</td>\n",
              "      <td>0.819936</td>\n",
              "      <td>0.741279</td>\n",
              "      <td>0.778626</td>\n",
              "      <td>0.938126</td>\n",
              "      <td>0.960857</td>\n",
              "      <td>0.949355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.031800</td>\n",
              "      <td>0.296236</td>\n",
              "      <td>0.899754</td>\n",
              "      <td>0.843750</td>\n",
              "      <td>0.861366</td>\n",
              "      <td>0.843750</td>\n",
              "      <td>0.850922</td>\n",
              "      <td>0.819444</td>\n",
              "      <td>0.846890</td>\n",
              "      <td>0.832941</td>\n",
              "      <td>0.831081</td>\n",
              "      <td>0.715116</td>\n",
              "      <td>0.768750</td>\n",
              "      <td>0.933573</td>\n",
              "      <td>0.969245</td>\n",
              "      <td>0.951075</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=7316, training_loss=0.07631562341426226, metrics={'train_runtime': 1176.9706, 'train_samples_per_second': 49.724, 'train_steps_per_second': 6.216, 'total_flos': 6015019398091200.0, 'train_loss': 0.07631562341426226, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjLpcIedn7y8",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kohQAMgzoJvt"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    outputs = model(**inputs)\n",
        "    prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "\n",
        "    if prediction == 0:\n",
        "        return \"Negative\"\n",
        "    elif prediction == 1:\n",
        "        return \"Neutral\"\n",
        "    else:\n",
        "      return \"Good\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNNo0poPheomwBXEUhg3RCZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}